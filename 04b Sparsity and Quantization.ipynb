{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5109e7b",
   "metadata": {},
   "source": [
    "# Quantizing a Sparse Model (QAT)\n",
    "\n",
    "Quantization can be combined with the sparsity toolkit (`torch.ao.sparsity`) to achieve even better computational performance. There are two main ways of quantizing a sparse model:\n",
    "\n",
    "1. Post-Training Quantization: Quantizes a model that was already trained and sparsified\n",
    "2. Quantization-Aware Training: Trains the model with quantization and sparsity in mind\n",
    "\n",
    "In this notebook we will focus on the quantization-aware training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857c51b",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training\n",
    "\n",
    "Quantization-Aware Training (QAT)can be used in combination with the sparsity. The key is to run the `torch.quantization.prepare_qat` **BEFORE** the `sparsifier.prepare`. The reason is that the `prepare_qat` utility replaces the layers that need to be quantized, while the sparsifier needs to keep track of the model to compute the sparsity. By keeping the order as described, we are making sure that the sparsifier has the same layers that the quatization toolflow is modifying.\n",
    "\n",
    "### Step 1: Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88655cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): QuantStub()\n",
      "  (1): Linear(in_features=7, out_features=32, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (8): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.quantization as tq\n",
    "\n",
    "in_features = 7\n",
    "num_classes = 10\n",
    "\n",
    "def make_model():\n",
    "    model = nn.Sequential(\n",
    "        tq.QuantStub(),\n",
    "        nn.Linear(in_features, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, num_classes),\n",
    "        tq.DeQuantStub()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb7774",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the model for QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf2a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch/torch/quantization/quantize.py:244: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub()\n",
       "  (1): Linear(in_features=7, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=256, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (8): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tq.prepare_qat(model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a088e1",
   "metadata": {},
   "source": [
    "### Step 3: Create a sparsifier (and a scheduler if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c908032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao import sparsity\n",
    "\n",
    "sparse_config = [\n",
    "    {'module': model[1], 'sparsity_level': 0.7, 'sparse_block_shape': (1, 4), 'zeros_per_block': 4},\n",
    "    {'module': model[3], 'sparsity_level': 0.9, 'sparse_block_shape': (1, 4), 'zeros_per_block': 4},\n",
    "    # The following layers will take default parameters\n",
    "    model[5],\n",
    "]\n",
    "\n",
    "sparse_defaults = {\n",
    "    'sparsity_level': 0.8,\n",
    "    'sparse_block_shape': (1, 4),\n",
    "    'zeros_per_block': 4\n",
    "}\n",
    "\n",
    "# Create a sparsifier and attach a model to it\n",
    "sparsifier = sparsity.WeightNormSparsifier(**sparse_defaults)\n",
    "sparsifier.prepare(model, config=sparse_config)\n",
    "\n",
    "# Create a scheduler\n",
    "def stepping_lambda(epoch):\n",
    "    steps = [0.0, 0.5, 0.75, 1.0]\n",
    "    if epoch >= len(steps):\n",
    "        return 1.0\n",
    "    return steps[epoch]\n",
    "scheduler = sparsity.LambdaSL(sparsifier, stepping_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f514d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub()\n",
       "  (1): ParametrizedLinear(\n",
       "    in_features=7, out_features=32, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): FakeSparsity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): ReLU()\n",
       "  (3): ParametrizedLinear(\n",
       "    in_features=32, out_features=256, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): FakeSparsity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ReLU()\n",
       "  (5): ParametrizedLinear(\n",
       "    in_features=256, out_features=32, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): FakeSparsity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): ReLU()\n",
       "  (7): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (8): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice the model has both the parametrizations and FakeSparsity\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1bf0c",
   "metadata": {},
   "source": [
    "### Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8969c6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch  1 / 20... Loss: 2.31\n",
      "Running epoch  2 / 20... Loss: 2.31\n",
      "Running epoch  3 / 20... Loss: 2.31\n",
      "Running epoch  4 / 20... Loss: 2.30\n",
      "Running epoch  5 / 20... Loss: 2.30\n",
      "Running epoch  6 / 20... Loss: 2.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch/torch/_tensor.py:565: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /home/zafar/Git/pytorch-dev/pytorch/aten/src/ATen/native/BinaryOps.cpp:506.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch  7 / 20... Loss: 2.31\n",
      "Running epoch  8 / 20... Loss: 2.30\n",
      "Running epoch  9 / 20... Loss: 2.30\n",
      "Running epoch 10 / 20... Loss: 2.30\n",
      "Running epoch 11 / 20... Loss: 2.30\n",
      "Running epoch 12 / 20... Loss: 2.30\n",
      "Running epoch 13 / 20... Loss: 2.30\n",
      "Running epoch 14 / 20... Loss: 2.31\n",
      "Running epoch 15 / 20... Loss: 2.31\n",
      "Running epoch 16 / 20... Loss: 2.30\n",
      "Running epoch 17 / 20... Loss: 2.30\n",
      "Running epoch 18 / 20... Loss: 2.30\n",
      "Running epoch 19 / 20... Loss: 2.30\n",
      "Running epoch 20 / 20... Loss: 2.30\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x = torch.randn(128, in_features)\n",
    "    y = torch.randint(0, num_classes, size=(128,))\n",
    "    \n",
    "    y_hat = model(x)\n",
    "    loss = criterion(y_hat, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    sparsifier.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Running epoch {epoch + 1:>2} / 20... Loss: {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab09d89",
   "metadata": {},
   "source": [
    "### Step 5: Convert the model\n",
    "\n",
    "The model now finished training, and we can squash the masks before converting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae90913",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsifier.squash_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0c8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization as tq\n",
    "import torch.ao.nn.sparse.quantized as ao_qnn\n",
    "from torch.ao.nn.sparse.quantized.utils import LinearBlockSparsePattern\n",
    "\n",
    "# Step 2: Create custom mapping\n",
    "#         You can also use dynamic mapping here that maps to `ao.nn.sparse.quantized.dynamic.Linear`\n",
    "sparse_mapping = tq.get_default_static_quant_module_mappings()\n",
    "sparse_mapping[nn.Linear] = ao_qnn.Linear\n",
    "\n",
    "# Step 3: Convert the model\n",
    "with LinearBlockSparsePattern(1, 4):\n",
    "    tq.convert(model, inplace=True, mapping=sparse_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd74fa0",
   "metadata": {},
   "source": [
    "The model is now quantized and uses sparse quantized kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5f7075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): QuantStub()\n",
      "  (1): Linear(in_features=7, out_features=32, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (8): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207fd245",
   "metadata": {},
   "source": [
    "TODO: There is bug in conversion!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e509ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
