{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e598b6b",
   "metadata": {},
   "source": [
    "# Customizing the Sparsity Workflow\n",
    "\n",
    "In this section we show how to write your own sparsifier and sparsity scheduler. As this is independent of the model training/inference, we will not create a model in this section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4609b7e",
   "metadata": {},
   "source": [
    "## Custom Sparsifier\n",
    "\n",
    "Suppose you have a need for a custom sparsifier, that sets individual elements in a sparse tensor to zero if they are too far from the mean of that tensor (either too big or too small).\n",
    "\n",
    "$$\n",
    "w^\\star_{ij} =\n",
    "\\begin{cases}\n",
    "w_{ij} & \\text{if}~ w_{ij} \\stackrel{\\rightarrow}{\\approx} \\mathbb{E}(w) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In practice, we can rank all elements by their distance from the mean, and ramove the furthest ones.\n",
    "\n",
    "To implement a custom sparsifier, you need to implement two methods:\n",
    "\n",
    "- `__init__(self, **kwargs)` constructor that would spacify the arguments required for your logic. This has to call `super().__init__(defaults=...)`. That way we make sure that the default arguments are propagated in case there are some parts of the model that don't have sparsity configurations.\n",
    "- `update_mask(self, layer, **kwargs)` -- this method is where the main logic of changing a single layer is. Generally, you would want to have the same arguments to the kwargs as in the constructor. However, this is optional, as all the default configurations are passed through.\n",
    "\n",
    "While writing the `update_mask`, you can get access to the mask that needs to be modified using `layer.parametrizations.weight[0].mask`. In addition to that if you require the original waight (non-sparsified) when you compute the next mask, you can access the original weight using `layer.parametrizations.weight.original`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70084b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.ao.sparsity import BaseSparsifier\n",
    "\n",
    "class ClosestToMeanSparsifier(BaseSparsifier):\n",
    "    def __init__(self, sparsity_level):\n",
    "        defaults = {\n",
    "            'sparsity_level': sparsity_level\n",
    "        }\n",
    "        super().__init__(defaults=defaults)\n",
    "        \n",
    "    def update_mask(self, layer, sparsity_level, **kwargs):\n",
    "        # Step 1: get the weight and the mask from the parametrizations\n",
    "        mask = layer.parametrizations.weight[0].mask\n",
    "        weight = layer.parametrizations.weight.original\n",
    "        # Step 2: implement the mask update logic\n",
    "        ## Step 2a: Compute the mean and the distance\n",
    "        weight_flat = weight.flatten()\n",
    "        weight_mean = weight_flat.mean()\n",
    "        weight_distance_to_mean = (weight_flat - weight_mean).abs()\n",
    "        ## Step 2b: Rank the elements in the tensor\n",
    "        _, sorted_idx = torch.sort(weight_distance_to_mean)\n",
    "        threshold_idx = int(round(sparsity_level * len(sorted_idx)))\n",
    "        sorted_idx = sorted_idx[:threshold_idx]\n",
    "        ## Step 2c: Create a mask with the known zero elements\n",
    "        new_mask = torch.ones_like(mask)\n",
    "        new_mask = new_mask.flatten()\n",
    "        new_mask[sorted_idx] = 0\n",
    "        new_mask = new_mask.reshape(mask.shape)\n",
    "        # Step 3: Reassign back to the mask\n",
    "        mask.data = new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49478de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao import sparsity\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "model = nn.Sequential(nn.Linear(128, 128))\n",
    "\n",
    "sparsifier = ClosestToMeanSparsifier(sparsity_level=0.8)\n",
    "sparsifier.prepare(model, config=None)\n",
    "sparsifier.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f90078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level of sparsity: 80.00%\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = (model[0].weight == 0).float().mean()\n",
    "print(f\"Level of sparsity: {sparsity_level.item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da801e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to squash the mask\n",
    "sparsifier.squash_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4613b4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aca303",
   "metadata": {},
   "source": [
    "## Custom Scheduler\n",
    "\n",
    "Suppose you have a need for a custom scheduler, that sets sparsity level after some numbre of epochs, resets it to 0 after some more, and finally sets it again. Such a scheduler would have 3 epochs at which the sparsity level will either be set to 0.0 or some target sparsity.\n",
    "\n",
    "$$\n",
    "\\text{current sparsity} =\n",
    "\\begin{cases}\n",
    "0 & \\text{if epoch} <e_0 \\\\\n",
    "\\text{target sparsity} & \\text{if} ~e_0 < \\text{epoch} < e_1\\\\\n",
    "0 & \\text{if} ~e_1 < \\text{epoch} < e_2 \\\\\n",
    "\\text{target sparsity} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To implement a custom scheuler, you only need to implement the method `get_sl`, which would return the list of levels of sparsity. As part of the scheduler, you have access to the following members:\n",
    "\n",
    "- `self.last_epoch` shows what was the last epoch that the scheduler was called\n",
    "- `self.base_sl` -- list of all the target sparsity levels\n",
    "- `self.get_last_sl()` -- method that shows the last sparsity level that was updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4597848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.sparsity import BaseScheduler\n",
    "\n",
    "class OnOffSchedulerSL(BaseScheduler):\n",
    "    def __init__(self, sparsifier, epoch_points, **kwargs):\n",
    "        self.epoch_points = epoch_points\n",
    "        super().__init__(sparsifier, **kwargs)\n",
    "        \n",
    "    def get_sl(self):\n",
    "        if self.last_epoch < self.epoch_points[0]:\n",
    "            return [0.0] * len(self.base_sl)\n",
    "        elif self.epoch_points[0] <= self.last_epoch < self.epoch_points[1]:\n",
    "            return self.base_sl\n",
    "        elif self.epoch_points[1] <= self.last_epoch < self.epoch_points[2]:\n",
    "            return [0.0] * len(self.base_sl)\n",
    "        else:\n",
    "            return self.base_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc55dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(128, 128))\n",
    "\n",
    "sparsifier = ClosestToMeanSparsifier(sparsity_level=0.8)\n",
    "sparsifier.prepare(model, config=None)\n",
    "scheduler = OnOffSchedulerSL(sparsifier, epoch_points=[3, 6, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6fda611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level of sparsity @ epoch  0: 0.00%\n",
      "Level of sparsity @ epoch  1: 0.00%\n",
      "Level of sparsity @ epoch  2: 0.00%\n",
      "Level of sparsity @ epoch  3: 80.00%\n",
      "Level of sparsity @ epoch  4: 80.00%\n",
      "Level of sparsity @ epoch  5: 80.00%\n",
      "Level of sparsity @ epoch  6: 0.00%\n",
      "Level of sparsity @ epoch  7: 0.00%\n",
      "Level of sparsity @ epoch  8: 0.00%\n",
      "Level of sparsity @ epoch  9: 80.00%\n",
      "Level of sparsity @ epoch 10: 80.00%\n",
      "Level of sparsity @ epoch 11: 80.00%\n",
      "Level of sparsity @ epoch 12: 80.00%\n",
      "Level of sparsity @ epoch 13: 80.00%\n",
      "Level of sparsity @ epoch 14: 80.00%\n"
     ]
    }
   ],
   "source": [
    "for idx in range(15):\n",
    "    sparsifier.step()\n",
    "    scheduler.step()\n",
    "    sparsity_level = (model[0].weight == 0).float().mean()\n",
    "    print(f\"Level of sparsity @ epoch {idx:>2}: {sparsity_level.item():0.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2eee4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
