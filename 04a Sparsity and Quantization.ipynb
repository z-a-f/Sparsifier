{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5109e7b",
   "metadata": {},
   "source": [
    "# Quantizing a Sparse Model (PTQ)\n",
    "\n",
    "Quantization can be combined with the sparsity toolkit (`torch.ao.sparsity`) to achieve even better computational performance. There are two main ways of quantizing a sparse model:\n",
    "\n",
    "1. Post-Training Quantization: Quantizes a model that was already trained and sparsified\n",
    "2. Quantization-Aware Training: Trains the model with quantization and sparsity in mind\n",
    "\n",
    "In this notebook we will focus on the post-training quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857c51b",
   "metadata": {},
   "source": [
    "## Post-Training Quantization\n",
    "\n",
    "This is the simplest way of quantizing a sparse model, as the quantization and sparsification are independent of each other. The general workflow is:\n",
    "\n",
    "1. Train the sparse model / Sparsify an existing model\n",
    "2. Squash the sparsity masks\n",
    "3. Quantize the model as if no sparsity was present\n",
    "\n",
    "Below we will follow a more detailed flow step-by-step\n",
    "\n",
    "### Step 1: Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88655cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): QuantStub()\n",
      "  (1): Linear(in_features=7, out_features=32, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (8): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.quantization as tq\n",
    "\n",
    "in_features = 7\n",
    "num_classes = 10\n",
    "\n",
    "def make_model():\n",
    "    model = nn.Sequential(\n",
    "        tq.QuantStub(),\n",
    "        nn.Linear(in_features, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, num_classes),\n",
    "        tq.DeQuantStub()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb7774",
   "metadata": {},
   "source": [
    "### Step 2: Attach the model to a sparsifier and step\n",
    "\n",
    "*Note: At this step you can follow the \"sparse training\" flow without thinking about the quantization just yet.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97173dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch/torch/_tensor.py:565: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /home/zafar/Git/pytorch-dev/pytorch/aten/src/ATen/native/BinaryOps.cpp:506.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from torch.ao import sparsity\n",
    "\n",
    "# Create a sparsifier\n",
    "\n",
    "sparse_config = [\n",
    "    {'module': model[1], 'sparsity_level': 0.7, 'sparse_block_shape': (1, 4), 'zeros_per_block': 4},\n",
    "    {'module': model[3], 'sparsity_level': 0.9, 'sparse_block_shape': (1, 4), 'zeros_per_block': 4},\n",
    "    # The following layers will take default parameters\n",
    "    model[5],\n",
    "]\n",
    "\n",
    "sparse_defaults = {\n",
    "    'sparsity_level': 0.8,\n",
    "    'sparse_block_shape': (1, 4),\n",
    "    'zeros_per_block': 4\n",
    "}\n",
    "\n",
    "# Create a sparsifier and attach a model to it\n",
    "sparsifier = sparsity.WeightNormSparsifier(**sparse_defaults)\n",
    "sparsifier.prepare(model, config=sparse_config)\n",
    "sparsifier.step()  # Sparsify the model\n",
    "sparsifier.squash_mask()\n",
    "\n",
    "# Save the model for future benchmarking\n",
    "model_fp = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ea151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in model[1]: 70.54%\n",
      "Sparsity in model[3]: 89.99%\n",
      "Sparsity in model[5]: 79.98%\n",
      "Sparsity in model[7]: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Show the sparsities achieved\n",
    "for name, m in model.named_modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        sparsity_level = (m.weight == 0).float().mean()\n",
    "        print(f'Sparsity in model[{name}]: {sparsity_level:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab09d89",
   "metadata": {},
   "source": [
    "### Step 3: Quantize the model\n",
    "\n",
    "Now that the sparse model is created, we can just run the post-training quantization. Note that we have implemented optimized sparse kernels, located at `torch.ao.nn`.\n",
    "\n",
    "The quantization sub-flow is as follows:\n",
    "\n",
    "1. Preapare and calibrate the model\n",
    "1. Create a custom mapping for the quantized kernels\n",
    "    - The mapping should be from `nn.Linear` to `ao.nn.sparse.quantized.Linear`.\n",
    "    - This step makes sure that we are utilizing the accelerated sparse-quantized kernels instead of just quantized kernels\n",
    "1. Use an existing `torch.quantization.convert` with `mapping` argument to quantize the model\n",
    "    - **Note:** We currently have a temporary measure to communicate the shapes of the zero blocks using a context manager (`torch.ao.nn.sparse.quantized.utils.LinearBlockSparsePattern`). This will be removed in the nearest future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0c8049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch/torch/quantization/observer.py:135: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.quantization as tq\n",
    "import torch.ao.nn.sparse.quantized as ao_qnn\n",
    "from torch.ao.nn.sparse.quantized.utils import LinearBlockSparsePattern\n",
    "\n",
    "model_qsp = copy.deepcopy(model)\n",
    "\n",
    "# Step 1. Prepare and calibrate\n",
    "model_qsp.qconfig = tq.get_default_qconfig()\n",
    "tq.prepare(model_qsp, inplace=True)\n",
    "model_qsp(torch.randn(128, in_features));\n",
    "\n",
    "# Step 2: Create custom mapping\n",
    "#         You can also use dynamic mapping here that maps to `ao.nn.sparse.quantized.dynamic.Linear`\n",
    "sparse_mapping = tq.get_default_static_quant_module_mappings()\n",
    "sparse_mapping[nn.Linear] = ao_qnn.Linear\n",
    "\n",
    "# Step 3: Convert the model\n",
    "with LinearBlockSparsePattern(1, 4):\n",
    "    tq.convert(model_qsp, inplace=True, mapping=sparse_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd74fa0",
   "metadata": {},
   "source": [
    "The model is now quantized and uses sparse quantized kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de5f7075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Quantize(scale=tensor([0.0458]), zero_point=tensor([64]), dtype=torch.quint8)\n",
      "  (1): SparseQuantizedLinear(in_features=7, out_features=32, scale=0.031617671251297, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "  (2): ReLU()\n",
      "  (3): SparseQuantizedLinear(in_features=32, out_features=256, scale=0.01034998707473278, zero_point=74, qscheme=torch.per_channel_affine)\n",
      "  (4): ReLU()\n",
      "  (5): SparseQuantizedLinear(in_features=256, out_features=32, scale=0.0019278707914054394, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "  (6): ReLU()\n",
      "  (7): SparseQuantizedLinear(in_features=32, out_features=10, scale=0.0022666696459054947, zero_point=77, qscheme=torch.per_channel_affine)\n",
      "  (8): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_qsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2f923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
