{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ad7f4b",
   "metadata": {},
   "source": [
    "# Sparsifying a Pre-Trained Model for Inference\n",
    "\n",
    "In this document we show how to prune a model using the `torch.ao.sparsity` toolkit.\n",
    "\n",
    "Before going into details, let us define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32a054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.ao import sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcfdb996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def make_model():\n",
    "    model = nn.Sequential(\n",
    "        nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf29445",
   "metadata": {},
   "source": [
    "Given the model above, here are the requirements for its pruning:\n",
    "\n",
    "- `model[0][0]`:\n",
    "    - `sparsity_level = 0.7`\n",
    "    - `sparse_block_shape = (4, 1)`\n",
    "- `model[0][2]`:\n",
    "    - `sparsity_level = 0.9`\n",
    "    - `sparse_block_shape = (1, 8)`\n",
    "- All other `nn.Linear` layers:\n",
    "    - `sparsity_level = 0.8`\n",
    "    - `sparse_block_shape = (1, 4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4431120",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "\n",
    "sparse_config = [\n",
    "    {'module': model[0][0], 'sparsity_level': 0.7, 'sparse_block_shape': (4, 1), 'zeros_per_block': 4},\n",
    "    {'module': model[0][2], 'sparsity_level': 0.9, 'sparse_block_shape': (1, 8), 'zeros_per_block': 8},\n",
    "    # The following layers will take default parameters\n",
    "    model[1],\n",
    "    model[3]\n",
    "]\n",
    "\n",
    "sparse_defaults = {\n",
    "    'sparsity_level': 0.8,\n",
    "    'sparse_block_shape': (1, 4),\n",
    "    'zeros_per_block': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69484f",
   "metadata": {},
   "source": [
    "## Step 1. Create a sparsifier\n",
    "\n",
    "Before we can attach a model to the sparsifier, we have to create a sparsifier with the defaults arguments. Notice that although the sparsifier is instantiated, it has no layers attached to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0344a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsifier = sparsity.WeightNormSparsifier(**sparse_defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac582c51",
   "metadata": {},
   "source": [
    "That creates a sparsifier and gives it the defaults that it could apply to the layers that don't have sparsity configured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2b559",
   "metadata": {},
   "source": [
    "## Step 2. Prepare the model for sparsification\n",
    "\n",
    "Now that the sparsifier is instantiated, we need to attach a model to it.\n",
    "\n",
    "**Note:** Once you `prepare` the model, it is modified by attaching weight transformations to it. If you need the original model, you need to deepcopy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947edc9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sparsifier.prepare(model, config=sparse_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61423b3e",
   "metadata": {},
   "source": [
    "After running the `prepare`, bother the sparsifier and the model is modified, such that the sparsifier would have information about the model, while the model would have mechanisms to apply the sparsity to the appropriate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e9b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> First sparsified group:\n",
      "{'sparsity_level': 0.7, 'sparse_block_shape': (4, 1), 'zeros_per_block': 4, 'module': ParametrizedLinear(\n",
      "  in_features=128, out_features=1024, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): FakeSparsity()\n",
      "    )\n",
      "  )\n",
      "), 'fqn': '0.0'}\n",
      "\n",
      "===> First linear layer:\n",
      "ParametrizedLinear(\n",
      "  in_features=128, out_features=1024, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): FakeSparsity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "===> Sparsity mask for the first linear layer:\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"===> First sparsified group:\")\n",
    "print(sparsifier.module_groups[0])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"===> First linear layer:\")\n",
    "print(model[0][0])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"===> Sparsity mask for the first linear layer:\")\n",
    "print(sparsifier.module_groups[0]['module'].parametrizations.weight[0].mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953a76c",
   "metadata": {},
   "source": [
    "## Step 3. Take a Sparsification Step\n",
    "\n",
    "Now that the sparsifier is aware of the model and the model has the weight parametrizations attached to it, you can take a step that will compute the sparsity masks. Although you can call the `step` method as many times as you want, in this example, we will cal it once to show how it affects the weight tensors within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe31a0b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in layer 0.0 = 0.00% (target = 70.00%, has_mask = True)\n",
      "Sparsity in layer 0.2 = 0.00% (target = 90.00%, has_mask = True)\n",
      "Sparsity in layer 1 = 0.00% (target = 80.00%, has_mask = True)\n",
      "Sparsity in layer 3 = 0.00% (target = 80.00%, has_mask = True)\n"
     ]
    }
   ],
   "source": [
    "# Show the level of sparsity BEFORE step:\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        weight_sparsity_level = (layer.weight == 0).float().mean()\n",
    "        sparsity_target = [mg['sparsity_level'] for mg in sparsifier.module_groups if mg['fqn'] == name][0]\n",
    "        has_mask = hasattr(layer, 'parametrizations')\n",
    "        print(f'Sparsity in layer {name} = {weight_sparsity_level:.2%} (target = {sparsity_target:.2%}, has_mask = {has_mask})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d9b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zafar/Git/pytorch-dev/pytorch/torch/_tensor.py:565: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /home/zafar/Git/pytorch-dev/pytorch/aten/src/ATen/native/BinaryOps.cpp:506.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "# Take a step\n",
    "sparsifier.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34e6e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in layer 0.0 = 70.00% (target = 70.00%)\n",
      "Sparsity in layer 0.2 = 90.00% (target = 90.00%)\n",
      "Sparsity in layer 1 = 80.00% (target = 80.00%)\n",
      "Sparsity in layer 3 = 80.00% (target = 80.00%)\n"
     ]
    }
   ],
   "source": [
    "# Show the level of sparsity AFTER step:\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        weight_sparsity_level = (layer.weight == 0).float().mean()\n",
    "        sparsity_target = [mg['sparsity_level'] for mg in sparsifier.module_groups if mg['fqn'] == name][0]\n",
    "        print(f'Sparsity in layer {name} = {weight_sparsity_level:.2%} (target = {sparsity_target:.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0c68a",
   "metadata": {},
   "source": [
    "## Step 4. \"Squash\" the Mask into the Weight\n",
    "\n",
    "Now that the mask is computed, and the model is ready to be deployed for inference, we can get rid of the mask tensor, and \"squash\" it into the weight tensor.\n",
    "This is achieved using the sparsifier's `.squash_mask()` method.\n",
    "\n",
    "To demonstrate the difference that the squashing makes, we can save the model, and check its size before and after squashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "367ec96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size BEFORE squashing: 3.43MB\n"
     ]
    }
   ],
   "source": [
    "# Save and check the size\n",
    "torch.save(model.state_dict(), \"model_before_squash.pt\")\n",
    "model_size = os.stat(\"model_before_squash.pt\").st_size\n",
    "print(f'Model size BEFORE squashing: {model_size / 1_000_000:.2f}MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc3f3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsifier.squash_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec27496",
   "metadata": {},
   "source": [
    "Notice that \"squashing\" multiplies the mask by the weight and deletes the mask after that. If you would like to keep the mask, you would need to make a copy before squashing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "888ef542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size AFTER squashing: 1.72MB\n"
     ]
    }
   ],
   "source": [
    "# Save and check the size\n",
    "torch.save(model.state_dict(), \"model_after_squash.pt\")\n",
    "model_size = os.stat(\"model_after_squash.pt\").st_size\n",
    "print(f'Model size AFTER squashing: {model_size / 1_000_000:.2f}MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34f5ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in layer 0.0 = 70.00% (has mask = False)\n",
      "Sparsity in layer 0.2 = 90.00% (has mask = False)\n",
      "Sparsity in layer 1 = 80.00% (has mask = False)\n",
      "Sparsity in layer 3 = 80.00% (has mask = False)\n"
     ]
    }
   ],
   "source": [
    "# Show the sparsity per layer\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        weight_sparsity_level = (layer.weight == 0).float().mean()\n",
    "        has_mask = hasattr(layer, 'parametrizations')\n",
    "        print(f'Sparsity in layer {name} = {weight_sparsity_level:.2%} (has mask = {has_mask})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e2aa8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
